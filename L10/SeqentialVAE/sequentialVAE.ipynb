{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1196f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98877338",
   "metadata": {},
   "source": [
    "## Environment\n",
    "Here, we are going to use a simple environment to test our model. Suppose we control a particle (represented by white rectangular) that moves in a 2D plane. We can directly control the velocity of the particle along $x$ and $y$ axis. The task is to control the particle moving towards a goal position (represented by red rectangular) at the center of the plane (see video below). \n",
    "\n",
    "<img src=\"./particle_sys_gif.gif\" width=200 height=200 />\n",
    "\n",
    "Now, assume we can not directly observe the position of the particle and the goal, but we can access to some visual observation that is represented as images (see figure below).\n",
    "\n",
    "<img src=\"./particle_sys.png\" width=150 height=150 />\n",
    "\n",
    "Now we are going to learn a state space model of this environment. Given the learnt model, we will be able to forward simulate trajectoris in latent space and plan some actions that minimize the distance to the goal position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82477e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleEnv:\n",
    "    def __init__(self):\n",
    "        self.state = np.array([0, 0])\n",
    "        self.goal_state = np.array([0.5, 0.5])\n",
    "\n",
    "        self.action_scale = 0.025\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "        self.state = 0.8 * (np.random.rand(2) - 0.5) + 0.5\n",
    "\n",
    "        reward = -np.sqrt(np.square(self.state - self.goal_state).sum())\n",
    "        state_image = self.generate_image()\n",
    "\n",
    "        # potential_goal_states = [[0.0, 0.0], [0.9, 0.9], [0.9, 0.0], [0.0, 0.9]]\n",
    "        # self.goal_state = np.array(potential_goal_states[random.choice([0, 1, 2, 3])])\n",
    "        return self.state.copy(), state_image, reward\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state += action * self.action_scale\n",
    "        self.state = np.clip(self.state, a_min=0.0, a_max=1.0)\n",
    "\n",
    "        reward = -np.sqrt(np.square(self.state - self.goal_state).sum())\n",
    "\n",
    "        if np.sqrt(np.square(self.state - self.goal_state).sum()) < 0.01:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        state_image = self.generate_image()\n",
    "        self.step_count += 1\n",
    "\n",
    "        if self.step_count > 10:\n",
    "            done = True\n",
    "        return self.state.copy(), state_image, reward, done\n",
    "\n",
    "    def generate_image(self):\n",
    "        resolution = 32\n",
    "        radius = 3\n",
    "        image_canvas = np.zeros(shape=[3, resolution, resolution])\n",
    "\n",
    "        pixel_x = int(self.state[0].item() * (resolution - 1))\n",
    "        pixel_y = int(self.state[1].item() * (resolution - 1))\n",
    "\n",
    "        for i in range(radius):\n",
    "            for j in range(radius):\n",
    "                image_canvas[:, np.clip(pixel_x + i, 0, resolution - 1), np.clip(pixel_y + j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[:, np.clip(pixel_x - i, 0, resolution - 1), np.clip(pixel_y - j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[:, np.clip(pixel_x + i, 0, resolution - 1), np.clip(pixel_y - j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[:, np.clip(pixel_x - i, 0, resolution - 1), np.clip(pixel_y + j, 0, resolution - 1)] = 1.0\n",
    "\n",
    "        pixel_x = int(self.goal_state[0].item() * (resolution - 1))\n",
    "        pixel_y = int(self.goal_state[1].item() * (resolution - 1))\n",
    "\n",
    "        for i in range(radius):\n",
    "            for j in range(radius):\n",
    "                image_canvas[0, np.clip(pixel_x + i, 0, resolution - 1), np.clip(pixel_y + j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[0, np.clip(pixel_x - i, 0, resolution - 1), np.clip(pixel_y - j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[0, np.clip(pixel_x + i, 0, resolution - 1), np.clip(pixel_y - j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[0, np.clip(pixel_x - i, 0, resolution - 1), np.clip(pixel_y + j, 0, resolution - 1)] = 1.0\n",
    "\n",
    "        return image_canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efa96c",
   "metadata": {},
   "source": [
    "Here, we define a ReplayBuffer to collect data when interacting with the environment. The ReplayBuffer will record the visual observations (images), actions, reward (negative distance to the goal) and terminal flag at each time step. We also define some neural network template that will be used later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6fd24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store and replay environment transitions.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_shape, action_shape, reward_shape, capacity, batch_size, length, device='cpu'):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.length = length\n",
    "        self.device = device\n",
    "        # Initialize all the buffers\n",
    "        self.obs_buffer = np.empty(shape=(capacity, *obs_shape), dtype=np.float32)\n",
    "        self.action_buffer = np.empty(shape=(capacity, *action_shape), dtype=np.float32)\n",
    "        self.reward_buffer = np.empty(shape=(capacity, *reward_shape), dtype=np.float32)\n",
    "        self.done_buffer = np.empty(shape=(capacity, *reward_shape), dtype=np.float32)\n",
    "        self.idx = 0\n",
    "\n",
    "    def add(self, obs, action, reward, done):\n",
    "        if self.idx < self.capacity:\n",
    "            self.obs_buffer[self.idx] = obs\n",
    "            self.action_buffer[self.idx] = action\n",
    "            self.reward_buffer[self.idx] = reward\n",
    "            self.done_buffer[self.idx] = done\n",
    "            self.idx += 1\n",
    "        else:\n",
    "            self.obs_buffer = self.obs_buffer[1:]\n",
    "            self.obs_buffer = np.append(self.obs_buffer,\n",
    "                                        obs.reshape((1, obs.shape[0], obs.shape[1], obs.shape[2])),\n",
    "                                        axis=0)\n",
    "            self.action_buffer = self.action_buffer[1:]\n",
    "            self.action_buffer = np.append(self.action_buffer,\n",
    "                                           action.reshape((1, action.shape[0])),\n",
    "                                           axis=0)\n",
    "            self.reward_buffer = self.reward_buffer[1:]\n",
    "            self.reward_buffer = np.append(self.reward_buffer,\n",
    "                                           reward.reshape((1, 1)),\n",
    "                                           axis=0)\n",
    "            self.done_buffer = self.done_buffer[1:]\n",
    "            self.done_buffer = np.append(self.done_buffer,\n",
    "                                         done.reshape((1, done.shape[0])),\n",
    "                                         axis=0)\n",
    "\n",
    "    def sample(self):\n",
    "        idxs = np.random.randint(\n",
    "            0, self.capacity - self.length + 1 if self.idx == self.capacity else self.idx - self.length + 1,\n",
    "            size=self.batch_size)\n",
    "        obses = torch.as_tensor(self.obs_buffer[idxs], device=self.device).unsqueeze(1).float()\n",
    "        actions = torch.as_tensor(self.action_buffer[idxs], device=self.device).unsqueeze(1).float()\n",
    "        rewards = torch.as_tensor(self.reward_buffer[idxs], device=self.device).unsqueeze(1).float()\n",
    "        dones = torch.as_tensor(self.done_buffer[idxs], device=self.device).unsqueeze(1).float()\n",
    "\n",
    "        for i in range(1, self.length):\n",
    "            next_obses = torch.as_tensor(self.obs_buffer[idxs + i], device=self.device).unsqueeze(1).float()\n",
    "            next_actions = torch.as_tensor(self.action_buffer[idxs + i], device=self.device).unsqueeze(1).float()\n",
    "            next_rewards = torch.as_tensor(self.reward_buffer[idxs + i], device=self.device).unsqueeze(1).float()\n",
    "            next_dones = torch.as_tensor(self.done_buffer[idxs + i], device=self.device).unsqueeze(1).float()\n",
    "            obses = torch.cat((obses, next_obses), 1)\n",
    "            actions = torch.cat((actions, next_actions), 1)\n",
    "            rewards = torch.cat((rewards, next_rewards), 1)\n",
    "            dones = torch.cat((dones, next_dones), 1)\n",
    "\n",
    "        return obses, actions, rewards, dones\n",
    "\n",
    "\n",
    "class CNNDenseModel(nn.Module):\n",
    "    def __init__(self, embed_dim: int, layers: int, h_dim: int,\n",
    "                 activation=nn.ReLU, min=1e-4, max=10.0):\n",
    "        super().__init__()\n",
    "        self._embed_size = embed_dim\n",
    "        self._layers = layers\n",
    "        self._hidden_size = h_dim\n",
    "        self.activation = activation\n",
    "        self.model = self.build_model()\n",
    "        self.soft_plus = nn.Softplus()\n",
    "        self._min = min\n",
    "        self._max = max\n",
    "        self.conv_channels = 4\n",
    "\n",
    "        self.conv_in = nn.Sequential(torch.nn.Conv2d(in_channels=3,\n",
    "                                                     out_channels=self.conv_channels,\n",
    "                                                     kernel_size=3,\n",
    "                                                     stride=3))\n",
    "        self.fc_out = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = [nn.Linear(400, self._hidden_size)]\n",
    "        model += [self.activation()]\n",
    "        for i in range(self._layers - 1):\n",
    "            model += [nn.Linear(self._hidden_size, self._hidden_size)]\n",
    "            model += [self.activation()]\n",
    "        model += [nn.Linear(self._hidden_size, self._embed_size)]\n",
    "        return nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, obs_visual):\n",
    "        x_visual = self.conv_in(obs_visual)\n",
    "        x_visual = x_visual.contiguous()\n",
    "        x_visual = x_visual.view(-1, self.conv_channels * 10 * 10)\n",
    "        x = self.fc_out(x_visual)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNDecoder(torch.nn.Module):\n",
    "    def __init__(self, z_dim=10, h_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv_channels = 4\n",
    "\n",
    "        self.fc = nn.Sequential(torch.nn.Linear(z_dim, h_dim),\n",
    "                                torch.nn.ReLU(),\n",
    "                                torch.nn.Linear(h_dim, self.conv_channels * 10 * 10))\n",
    "        self.deconv = nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(in_channels=self.conv_channels, out_channels=3, kernel_size=5, stride=3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        h = x.view(-1, self.conv_channels, 10, 10)\n",
    "        h = self.deconv(h)  # , output_size=(x.size(0), 3, 28, 28))\n",
    "        return h\n",
    "\n",
    "\n",
    "class DenseModelNormal(nn.Module):\n",
    "    def __init__(self, feature_dim: int, output_shape: tuple, layers: int, h_dim: int, activation=nn.ELU,\n",
    "                 min=1e-4, max=10.0):\n",
    "        super().__init__()\n",
    "        self._output_shape = output_shape\n",
    "        self._layers = layers\n",
    "        self._hidden_size = h_dim\n",
    "        self.activation = activation\n",
    "        # For adjusting pytorch to tensorflow\n",
    "        self._feature_size = feature_dim\n",
    "        # Defining the structure of the NN\n",
    "        self.model = self.build_model()\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "        self._min = min\n",
    "        self._max = max\n",
    "\n",
    "    def build_model(self):\n",
    "        model = [nn.Linear(self._feature_size, self._hidden_size)]\n",
    "        model += [self.activation()]\n",
    "        for i in range(self._layers - 1):\n",
    "            model += [nn.Linear(self._hidden_size, self._hidden_size)]\n",
    "            model += [self.activation()]\n",
    "        model += [nn.Linear(self._hidden_size, 2 * int(np.prod(self._output_shape)))]\n",
    "        return nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, features):\n",
    "        dist_inputs = self.model(features)\n",
    "        reshaped_inputs_mean = torch.reshape(dist_inputs[..., :np.prod(self._output_shape)],\n",
    "                                             features.shape[:-1] + self._output_shape)\n",
    "        reshaped_inputs_std = torch.reshape(dist_inputs[..., np.prod(self._output_shape):],\n",
    "                                            features.shape[:-1] + self._output_shape)\n",
    "\n",
    "        reshaped_inputs_std = torch.clamp(self.soft_plus(reshaped_inputs_std), min=self._min, max=self._max)\n",
    "        return td.independent.Independent(td.Normal(reshaped_inputs_mean, reshaped_inputs_std), len(self._output_shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43120543",
   "metadata": {},
   "source": [
    "## State Space Model\n",
    "Here, we define our State Space Model (SSM). Intuitively, the SSM models an agent that is sequentially taking actions in a world and receiving rewards and visual observations. The observation $x_t$ at time $t$ visual observations are generated from the latent state $z_t$. The model assumes Markovian transitions where the next state is conditioned upon the current state and the action $a_t$ taken by the agent. Upon taking an action, the agent receives reward $r_t$. Given the graphical structure in the figure below, the SSM's joint distribution factorizes as:\n",
    "\n",
    "<img src=\"./pgm.png\" width=400 height=400 />\n",
    "\n",
    "\\begin{align}\n",
    "    &p_\\theta(x_{1:T},r_{1:T},z_{0:T},a_{1:T-1}) = \\prod_{t=1}^{T} p_\\theta(x_{t}|z_{t})p_\\theta(r_{t}|z_{t})p_\\theta(z_{t}|z_{t-1},a_{t-1})p_\\psi(a_{t-1}|z_{t-1})p(z_0)\n",
    "\\end{align}\n",
    "\n",
    "Each of the factorized distributions are modelled using nonlinear functions:\n",
    "\n",
    "* Transitions: $p_\\theta(z_{t}|z_{t-1},a_{t-1}) = p(z_{t}| f_\\theta(z_{t-1},a_{t-1}))$\n",
    "* Observations: $p_\\theta(x_{t}|z_{t}) = p(x_{t}| d_\\theta(z_{t})) $ \n",
    "* Rewards: $p_\\theta(r_{t}|z_{t}) = p(r_{t}| r_\\theta(z_{t})) $\n",
    "\n",
    "where $f_\\theta$, $d^m_\\theta$, $r_\\theta$, and $\\pi_\\psi$ are neural networks parameterized by $\\theta$. Given nonlinearity of these factorized distributions, the posterior distribution $p(z_{1:T}|x_{1:T},a_{1:T-1})$ is intractable. Thus, we approximate it by \n",
    "\n",
    "$$p(z_{1:T}|x_{1:T},a_{1:T-1})=\\prod_{t=2}^T q_\\phi(z_{t}|g_\\phi(x_{t}, z_{t-1}, a_{t-1}))p(z_1)$$\n",
    "\n",
    "where $q_\\phi$ is modeled as a Gaussian distribution and $g_\\theta(x_{t}, z_{t-1}, a_{t-1})$ is a neural network parameterized by $\\phi$, which is typically called inference network.\n",
    "\n",
    "Given all these distributions and trajectories of the form $\\tau = \\left\\{(x_{t}, a_{t}, r_{t})\\right\\}_{t=1}^{T}$ that samples from the ReplayBuffer, we seek to learn the parameters $\\theta$ and $\\phi$. Because maximum likelihood estimation is intractable in this setting, we optimize the evidence lower bound (ELBO) under the data distribution $p_d$ using a variational distribution $q$ over the latent state variables $z_t$ \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{p_d}[\\mathrm{ELBO}] \\leq \\mathbb{E}_{p_d}[\\log p_\\theta(x_{1:T},r_{1:T}|a_{1:T-1})]\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\\begin{align}\n",
    "     \\mathrm{ELBO} = & \\sum_{t=1}^{T}\\Big(\\displaystyle  \\mathop{\\mathbb{E}}_{q_{\\phi}(z_{t})}\\left[\\sum_{m=1}^M\\log p_{\\theta}(x_{t}|z_{t})\\right] +\\displaystyle\\mathop{\\mathbb{E}}_{q_{\\phi}(z_{t})}\\left[\\log p_{\\theta}(r_{t}|z_{t})\\right]  \\nonumber\\\\\n",
    "    &- \\displaystyle\\mathop{\\mathbb{E}}_{q_{\\phi}(z_{t-1})}\\left[\\mathrm{KL}\\left[q_{\\phi}(z_{t}) \\| p_{\\theta}(z_{t}|z_{t-1},a_{t-1})\\right]\\right]\\Big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "777370d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialVAE(nn.Module):\n",
    "    def __init__(self, z_dim=8, action_dim=2, embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.image_embed_net = CNNDenseModel(embed_dim=embed_dim, h_dim=64, layers=2)\n",
    "        self.inference_net = DenseModelNormal(feature_dim=embed_dim + z_dim + action_dim,\n",
    "                                              output_shape=(z_dim,),\n",
    "                                              layers=2,\n",
    "                                              h_dim=32)\n",
    "\n",
    "        self.transition_net = DenseModelNormal(feature_dim=z_dim + action_dim,\n",
    "                                               output_shape=(z_dim,),\n",
    "                                               layers=2,\n",
    "                                               h_dim=32)\n",
    "\n",
    "        self.emission_net = CNNDecoder(z_dim=z_dim, h_dim=32)\n",
    "        self.reward_net = DenseModelNormal(feature_dim=z_dim,\n",
    "                                           output_shape=(1,),\n",
    "                                           layers=2,\n",
    "                                           h_dim=32)\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(params=self.parameters(), lr=0.002)\n",
    "\n",
    "        self.anneal_factor = 2.0\n",
    "\n",
    "    def get_z_post_dist(self, obs, z=None, action=None):\n",
    "\n",
    "        obs_embed = self.image_embed_net(obs)\n",
    "\n",
    "        if len(obs.shape) == 3:\n",
    "            obs_embed = obs_embed[0]\n",
    "\n",
    "        if z is None:\n",
    "            z = torch.zeros(obs_embed.shape[:-1] + (self.z_dim,))\n",
    "\n",
    "        if action is None:\n",
    "            action = torch.zeros(obs_embed.shape[:-1] + (self.action_dim,))\n",
    "        z_post_dist = self.inference_net(torch.cat([obs_embed, z, action], dim=-1))\n",
    "        return z_post_dist\n",
    "\n",
    "    def cross_entropy_planning(self, z):\n",
    "        horizon = 4\n",
    "        sample_size = 100\n",
    "        discount_factor = 0.8\n",
    "\n",
    "        action_samples = torch.randint(low=-1, high=2, size=(sample_size, horizon, self.action_dim))\n",
    "\n",
    "        accumulated_reward = torch.zeros((1, 1))\n",
    "        z_temp = z.clone().unsqueeze(0).repeat(sample_size, 1)\n",
    "        for t in range(horizon):\n",
    "            z_temp_dist = self.transition_net(torch.cat([z_temp, action_samples[:, t]], dim=-1))\n",
    "            z_temp = z_temp_dist.mean\n",
    "\n",
    "            reward = self.reward_net(z_temp).mean\n",
    "            accumulated_reward = accumulated_reward + reward * (discount_factor ** t)\n",
    "\n",
    "        opti_id = torch.argmax(accumulated_reward, dim=0)\n",
    "        opti_action_sample = action_samples[opti_id.item()]\n",
    "        return opti_action_sample[0]\n",
    "\n",
    "    def learn(self, replay_buffer, episode_count):\n",
    "        max_epoch = 200\n",
    "        for i in range(max_epoch):\n",
    "            obs, action, reward, done = replay_buffer.sample()\n",
    "\n",
    "            batch_t = obs.shape[1]\n",
    "\n",
    "            # get latent_state\n",
    "            z_post_rsample = [[]] * batch_t\n",
    "            z_post_mean = [[]] * batch_t\n",
    "            z_post_std = [[]] * batch_t\n",
    "\n",
    "            for t in range(batch_t):\n",
    "                if t == 0:\n",
    "                    z_post_dist = self.get_z_post_dist(obs[:, t])\n",
    "                else:\n",
    "                    z_post_dist = self.get_z_post_dist(obs[:, t], z_post_rsample[t - 1], action[:, t - 1])\n",
    "                z_post_rsample[t] = z_post_dist.rsample()\n",
    "                z_post_mean[t] = z_post_dist.mean\n",
    "                z_post_std[t] = z_post_dist.stddev\n",
    "            z_post_rsample = torch.stack(z_post_rsample, dim=1)\n",
    "            z_post_mean = torch.stack(z_post_mean, dim=1)\n",
    "            z_post_std = torch.stack(z_post_std, dim=1)\n",
    "\n",
    "            z_trans_mean = [[]] * batch_t\n",
    "            z_trans_std = [[]] * batch_t\n",
    "            for t in range(batch_t):\n",
    "                if t == 0:\n",
    "                    z_trans_mean[t] = z_post_mean[:, t]\n",
    "                    z_trans_std[t] = z_post_std[:, t]\n",
    "                else:\n",
    "                    z_trans_dist = self.transition_net(torch.cat([z_post_rsample[:, t - 1], action[:, t]], dim=-1))\n",
    "                    z_trans_mean[t] = z_trans_dist.mean\n",
    "                    z_trans_std[t] = z_trans_dist.stddev\n",
    "            z_trans_mean = torch.stack(z_trans_mean, dim=1)\n",
    "            z_trans_std = torch.stack(z_trans_std, dim=1)\n",
    "\n",
    "            obs_rec = self.emission_net(z_post_rsample)\n",
    "            obs_rec_loss = 100 * torch.square(obs_rec - obs.view(-1, 3, 32, 32)).mean()\n",
    "\n",
    "            reward_dist = self.reward_net(z_post_mean.detach())\n",
    "            reward_rec_loss = -reward_dist.log_prob(reward).mean()\n",
    "\n",
    "            kl_loss = (z_trans_std.log() - z_post_std.log()\n",
    "                       + (z_post_std.pow(2) + (z_trans_mean - z_post_mean).pow(2))\n",
    "                       / (2 * z_trans_std.pow(2) + 1e-5) - 0.5).mean()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = obs_rec_loss + reward_rec_loss + 1.0 * torch.max(kl_loss, torch.ones(1) * self.anneal_factor)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.anneal_factor = np.clip(self.anneal_factor * 0.9, 0.1, 10.0)\n",
    "\n",
    "        print(\n",
    "            f'{episode_count}: obs_loss:{obs_rec_loss.item()}, reward_loss:{reward_rec_loss.item()}, kl_loss:{kl_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335439b",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "Without loss of generality, we begin at time-step $t-1$ where we have a sample of $z_{t-1}$ and taken a action $a_{t-1}$. At time step $t$, we first recieve an visual observation $x_t$ and reward $r_t$. Then,we pass $z_{t-1}$, $a_{t-1}$ and $x_t$ into the variational distribution $q_\\phi(z_t|z_{t-1}, a_{t-1}, x_t)$ and sample a $z_t$ from $q_\\phi$. Given the sample $z_t$, we optimize the following objective:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{argmax}_{a_{t:t+H-1}} \\ \\ J = \\mathrm{E}_{p(z_{t+1:t+H}\\ \\ \\ \\ |a_{t:t+H-1}\\ \\ \\ \\ ,z_t)}\\left[\\sum_{k=t+1}^{t+H} {\\gamma^tr(z_t)} \\right]\n",
    "\\end{align}\n",
    "\n",
    "where $p(z_{t+1:t+H}|a_{t:t+H-1},z_t)=\\prod_{k=t+1}^{t+H}p_\\theta(z_{k}|f_\\theta(z_{k-1},a_{k-1}))$ We choose action $a_t$ to execute and replan at each time step. After that,environment fowards to the $t+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54029a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(test=False):\n",
    "    env = ParticleEnv()\n",
    "    replay_buffer = ReplayBuffer(obs_shape=(3, 32, 32),\n",
    "                                 action_shape=(2,),\n",
    "                                 reward_shape=(1,),\n",
    "                                 capacity=1000,\n",
    "                                 batch_size=50,\n",
    "                                 length=10)\n",
    "\n",
    "    model = SequentialVAE(z_dim=5, action_dim=2)\n",
    "    if test:\n",
    "        model.load_state_dict(torch.load('./model.pt'))\n",
    "\n",
    "    model.eval()\n",
    "    _, obs, reward = env.reset()\n",
    "\n",
    "    z_post_mean = None\n",
    "    action = None\n",
    "\n",
    "    max_episode = 100\n",
    "    episode_count = 0\n",
    "    episode_data_size = 200\n",
    "\n",
    "    visualize_freq = 5\n",
    "\n",
    "\n",
    "    ims = []\n",
    "    fig, ax = plt.subplots()\n",
    "    while True:\n",
    "        avg_reward = 0.0\n",
    "        for i in range(episode_data_size):\n",
    "            z_post_mean = model.get_z_post_dist(torch.as_tensor(obs).float(), z_post_mean, action).mean\n",
    "\n",
    "            action = model.cross_entropy_planning(z_post_mean)\n",
    "            _, obs, reward, done = env.step(action.detach().numpy())\n",
    "\n",
    "            avg_reward += reward\n",
    "            visualize_image = obs.transpose((1, 2, 0))\n",
    "            if episode_count % visualize_freq == 0:\n",
    "\n",
    "                im = ax.imshow(visualize_image, animated=True)\n",
    "                ims.append([im])\n",
    "\n",
    "            replay_buffer.add(obs, action, reward, np.array([done]))\n",
    "\n",
    "            if done:\n",
    "                _, obs, reward = env.reset()\n",
    "                z_post_mean = None\n",
    "                action = None\n",
    "\n",
    "        avg_reward /= episode_data_size\n",
    "        print(f'avg_reward:{avg_reward}')\n",
    "\n",
    "        if episode_count % visualize_freq == 0:\n",
    "            ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "                                            repeat_delay=1000)\n",
    "            writergif = animation.PillowWriter(fps=30)\n",
    "\n",
    "            if test:\n",
    "                file_name = f'./test_episode_{episode_count}.gif'\n",
    "            else:\n",
    "                file_name = f'./episode_{episode_count}.gif'\n",
    "            ani.save(file_name, writer=writergif)\n",
    "            ims.clear()\n",
    "\n",
    "        if not test:\n",
    "            # train model using collected data\n",
    "            model.train()\n",
    "            model.learn(replay_buffer, episode_count)\n",
    "            model.eval()\n",
    "            torch.save(model.state_dict(), './model.pt')\n",
    "\n",
    "        episode_count += 1\n",
    "        if episode_count > max_episode:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad8726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_reward:-0.38256699793870363\n",
      "0: obs_loss:1.7802104949951172, reward_loss:-0.9428811073303223, kl_loss:1.165176272392273\n",
      "avg_reward:-0.38967081489607147\n",
      "1: obs_loss:1.4218614101409912, reward_loss:-0.91034996509552, kl_loss:1.3015347719192505\n",
      "avg_reward:-0.3273090250466683\n",
      "2: obs_loss:1.2923067808151245, reward_loss:-1.0265989303588867, kl_loss:1.5137642621994019\n",
      "avg_reward:-0.32336307055400837\n",
      "3: obs_loss:1.2721248865127563, reward_loss:-1.1582387685775757, kl_loss:1.1997597217559814\n",
      "avg_reward:-0.23971112899905983\n",
      "4: obs_loss:1.1961008310317993, reward_loss:-1.254366397857666, kl_loss:1.2024930715560913\n",
      "avg_reward:-0.3180173344662827\n",
      "5: obs_loss:1.1998205184936523, reward_loss:-1.3241875171661377, kl_loss:1.0794649124145508\n",
      "avg_reward:-0.36725186314524655\n",
      "6: obs_loss:1.210439682006836, reward_loss:-1.334482192993164, kl_loss:0.9939008951187134\n",
      "avg_reward:-0.35315265823084063\n",
      "7: obs_loss:1.206045389175415, reward_loss:-1.3584407567977905, kl_loss:0.9151557683944702\n",
      "avg_reward:-0.23613933054095126\n",
      "8: obs_loss:1.144340991973877, reward_loss:-1.7264782190322876, kl_loss:0.829367995262146\n",
      "avg_reward:-0.257780499273821\n",
      "9: obs_loss:1.1763012409210205, reward_loss:-1.6872056722640991, kl_loss:0.7943227291107178\n",
      "avg_reward:-0.3096244340470012\n",
      "10: obs_loss:1.1921552419662476, reward_loss:-1.7765469551086426, kl_loss:0.6952340006828308\n",
      "avg_reward:-0.28851610910845765\n",
      "11: obs_loss:1.1312084197998047, reward_loss:-2.0922324657440186, kl_loss:0.6223173141479492\n",
      "avg_reward:-0.23252116163752767\n",
      "12: obs_loss:1.0537136793136597, reward_loss:-1.9714192152023315, kl_loss:0.5527812242507935\n",
      "avg_reward:-0.21994572743536675\n",
      "13: obs_loss:1.1031310558319092, reward_loss:-2.13642954826355, kl_loss:0.5299771428108215\n",
      "avg_reward:-0.2351537230067595\n",
      "14: obs_loss:1.0247743129730225, reward_loss:-2.135895013809204, kl_loss:0.4533371031284332\n",
      "avg_reward:-0.2639848135954856\n",
      "15: obs_loss:1.128153920173645, reward_loss:-2.269235372543335, kl_loss:0.3937060534954071\n",
      "avg_reward:-0.21989070034774735\n",
      "16: obs_loss:1.080178141593933, reward_loss:-2.189552068710327, kl_loss:0.3596796989440918\n",
      "avg_reward:-0.18207025387373268\n",
      "17: obs_loss:1.0374833345413208, reward_loss:-2.184328317642212, kl_loss:0.3826712369918823\n",
      "avg_reward:-0.20137679759298632\n",
      "18: obs_loss:0.9962535500526428, reward_loss:-2.376580238342285, kl_loss:0.330220103263855\n",
      "avg_reward:-0.22030616615081455\n",
      "19: obs_loss:0.9887673854827881, reward_loss:-2.5193252563476562, kl_loss:0.32988110184669495\n",
      "avg_reward:-0.24188062569497873\n",
      "20: obs_loss:1.0502008199691772, reward_loss:-2.4018454551696777, kl_loss:0.3034374415874481\n",
      "avg_reward:-0.19564963894019793\n",
      "21: obs_loss:0.9777553081512451, reward_loss:-2.485405445098877, kl_loss:0.29919910430908203\n",
      "avg_reward:-0.2533361947340291\n",
      "22: obs_loss:0.9505490064620972, reward_loss:-1.9321085214614868, kl_loss:0.29335176944732666\n",
      "avg_reward:-0.20317865272719637\n",
      "23: obs_loss:0.949547529220581, reward_loss:-2.4169046878814697, kl_loss:0.28749096393585205\n",
      "avg_reward:-0.2146569131095807\n",
      "24: obs_loss:0.9666271209716797, reward_loss:-2.3616855144500732, kl_loss:0.2810831069946289\n",
      "avg_reward:-0.24550638418624068\n",
      "25: obs_loss:0.9239546060562134, reward_loss:-2.4121506214141846, kl_loss:0.2840537130832672\n",
      "avg_reward:-0.2581817955809049\n",
      "26: obs_loss:0.9265215992927551, reward_loss:-2.5390939712524414, kl_loss:0.2812042832374573\n",
      "avg_reward:-0.18593308687843557\n",
      "27: obs_loss:1.0176891088485718, reward_loss:-2.5068447589874268, kl_loss:0.2788265347480774\n",
      "avg_reward:-0.2580443364927263\n",
      "28: obs_loss:0.9160783886909485, reward_loss:-2.4940109252929688, kl_loss:0.2571660280227661\n",
      "avg_reward:-0.19113951305733282\n",
      "29: obs_loss:1.0463836193084717, reward_loss:-2.143315315246582, kl_loss:0.2804035246372223\n",
      "avg_reward:-0.18586489014392207\n",
      "30: obs_loss:0.9979698657989502, reward_loss:-2.3861238956451416, kl_loss:0.28293517231941223\n",
      "avg_reward:-0.23891036261386667\n",
      "31: obs_loss:0.8660228252410889, reward_loss:-2.5656871795654297, kl_loss:0.24145427346229553\n",
      "avg_reward:-0.19232759093360027\n",
      "32: obs_loss:1.0012563467025757, reward_loss:-2.320906639099121, kl_loss:0.2241101861000061\n",
      "avg_reward:-0.1818380586843069\n",
      "33: obs_loss:0.8309189081192017, reward_loss:-2.5311005115509033, kl_loss:0.20263557136058807\n",
      "avg_reward:-0.21697553424420823\n",
      "34: obs_loss:0.8170225620269775, reward_loss:-2.5763940811157227, kl_loss:0.17690004408359528\n",
      "avg_reward:-0.22173230949727832\n",
      "35: obs_loss:0.8725423216819763, reward_loss:-2.4775390625, kl_loss:0.20536589622497559\n",
      "avg_reward:-0.18161668047544943\n",
      "36: obs_loss:0.8180681467056274, reward_loss:-2.7577579021453857, kl_loss:0.19360479712486267\n",
      "avg_reward:-0.18892682069377728\n",
      "37: obs_loss:0.8608975410461426, reward_loss:-2.6208958625793457, kl_loss:0.22923646867275238\n",
      "avg_reward:-0.21660395128884122\n",
      "38: obs_loss:0.9277653694152832, reward_loss:-2.522968292236328, kl_loss:0.18930795788764954\n",
      "avg_reward:-0.1637129789033752\n",
      "39: obs_loss:0.8888475894927979, reward_loss:-2.5602352619171143, kl_loss:0.2416476309299469\n",
      "avg_reward:-0.17436744062314705\n"
     ]
    }
   ],
   "source": [
    "main(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13f349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e510714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
