{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1196f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as td\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e391b03",
   "metadata": {},
   "source": [
    "## Environment\n",
    "Here, we are going to use a simple environment to test our model. Suppose we control a particle (represented by white rectangular) that moves in a 2D plane. We can directly control the velocity of the particle along $x$ and $y$ axis. The task is to control the particle moving towards a goal position (represented by red rectangular) at the center of the plane (see video below). \n",
    "\n",
    "<img src=\"./particle_sys_gif.gif\" width=200 height=200 />\n",
    "\n",
    "Now, assume we can not directly observe the position of the particle and the goal, but we can access to some visual observation that is represented as images (see figure below).\n",
    "\n",
    "<img src=\"./particle_sys.png\" width=150 height=150 />\n",
    "\n",
    "Now we are going to learn a state space model of this environment. Given the learnt model, we will be able to forward simulate trajectoris in latent space and plan some actions that minimize the distance to the goal position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82477e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleEnv:\n",
    "    def __init__(self):\n",
    "        self.state = np.array([0, 0])\n",
    "        self.goal_state = np.array([0.5, 0.5])\n",
    "\n",
    "        self.action_scale = 0.025\n",
    "        self.step_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "        self.state = 0.8 * (np.random.rand(2) - 0.5) + 0.5\n",
    "\n",
    "        reward = -np.sqrt(np.square(self.state - self.goal_state).sum())\n",
    "        state_image = self.generate_image()\n",
    "\n",
    "        # potential_goal_states = [[0.0, 0.0], [0.9, 0.9], [0.9, 0.0], [0.0, 0.9]]\n",
    "        # self.goal_state = np.array(potential_goal_states[random.choice([0, 1, 2, 3])])\n",
    "        return self.state.copy(), state_image, reward\n",
    "\n",
    "    def step(self, action):\n",
    "        self.state += action * self.action_scale\n",
    "        self.state = np.clip(self.state, a_min=0.0, a_max=1.0)\n",
    "\n",
    "        reward = -np.sqrt(np.square(self.state - self.goal_state).sum())\n",
    "\n",
    "        if np.sqrt(np.square(self.state - self.goal_state).sum()) < 0.01:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        state_image = self.generate_image()\n",
    "        self.step_count += 1\n",
    "\n",
    "        if self.step_count > 10:\n",
    "            done = True\n",
    "        return self.state.copy(), state_image, reward, done\n",
    "\n",
    "    def generate_image(self):\n",
    "        resolution = 32\n",
    "        radius = 3\n",
    "        image_canvas = np.zeros(shape=[3, resolution, resolution])\n",
    "\n",
    "        pixel_x = int(self.state[0].item() * (resolution - 1))\n",
    "        pixel_y = int(self.state[1].item() * (resolution - 1))\n",
    "\n",
    "        for i in range(radius):\n",
    "            for j in range(radius):\n",
    "                image_canvas[:, np.clip(pixel_x + i, 0, resolution - 1), np.clip(pixel_y + j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[:, np.clip(pixel_x - i, 0, resolution - 1), np.clip(pixel_y - j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[:, np.clip(pixel_x + i, 0, resolution - 1), np.clip(pixel_y - j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[:, np.clip(pixel_x - i, 0, resolution - 1), np.clip(pixel_y + j, 0, resolution - 1)] = 1.0\n",
    "\n",
    "        pixel_x = int(self.goal_state[0].item() * (resolution - 1))\n",
    "        pixel_y = int(self.goal_state[1].item() * (resolution - 1))\n",
    "\n",
    "        for i in range(radius):\n",
    "            for j in range(radius):\n",
    "                image_canvas[0, np.clip(pixel_x + i, 0, resolution - 1), np.clip(pixel_y + j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[0, np.clip(pixel_x - i, 0, resolution - 1), np.clip(pixel_y - j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[0, np.clip(pixel_x + i, 0, resolution - 1), np.clip(pixel_y - j, 0, resolution - 1)] = 1.0\n",
    "                image_canvas[0, np.clip(pixel_x - i, 0, resolution - 1), np.clip(pixel_y + j, 0, resolution - 1)] = 1.0\n",
    "\n",
    "        return image_canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b1b787",
   "metadata": {},
   "source": [
    "Here, we define a ReplayBuffer to collect data when interacting with the environment. The ReplayBuffer will record the visual observations (images), actions, reward (negative distance to the goal) and terminal flag at each time step. We also define some neural network template that will be used later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6fd24b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store and replay environment transitions.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_shape, action_shape, reward_shape, capacity, batch_size, length, device='cpu'):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.length = length\n",
    "        self.device = device\n",
    "        # Initialize all the buffers\n",
    "        self.obs_buffer = np.empty(shape=(capacity, *obs_shape), dtype=np.float32)\n",
    "        self.action_buffer = np.empty(shape=(capacity, *action_shape), dtype=np.float32)\n",
    "        self.reward_buffer = np.empty(shape=(capacity, *reward_shape), dtype=np.float32)\n",
    "        self.done_buffer = np.empty(shape=(capacity, *reward_shape), dtype=np.float32)\n",
    "        self.idx = 0\n",
    "\n",
    "    def add(self, obs, action, reward, done):\n",
    "        if self.idx < self.capacity:\n",
    "            self.obs_buffer[self.idx] = obs\n",
    "            self.action_buffer[self.idx] = action\n",
    "            self.reward_buffer[self.idx] = reward\n",
    "            self.done_buffer[self.idx] = done\n",
    "            self.idx += 1\n",
    "        else:\n",
    "            self.obs_buffer = self.obs_buffer[1:]\n",
    "            self.obs_buffer = np.append(self.obs_buffer,\n",
    "                                        obs.reshape((1, obs.shape[0], obs.shape[1], obs.shape[2])),\n",
    "                                        axis=0)\n",
    "            self.action_buffer = self.action_buffer[1:]\n",
    "            self.action_buffer = np.append(self.action_buffer,\n",
    "                                           action.reshape((1, action.shape[0])),\n",
    "                                           axis=0)\n",
    "            self.reward_buffer = self.reward_buffer[1:]\n",
    "            self.reward_buffer = np.append(self.reward_buffer,\n",
    "                                           reward.reshape((1, 1)),\n",
    "                                           axis=0)\n",
    "            self.done_buffer = self.done_buffer[1:]\n",
    "            self.done_buffer = np.append(self.done_buffer,\n",
    "                                         done.reshape((1, done.shape[0])),\n",
    "                                         axis=0)\n",
    "\n",
    "    def sample(self):\n",
    "        idxs = np.random.randint(\n",
    "            0, self.capacity - self.length + 1 if self.idx == self.capacity else self.idx - self.length + 1,\n",
    "            size=self.batch_size)\n",
    "        obses = torch.as_tensor(self.obs_buffer[idxs], device=self.device).unsqueeze(1).float()\n",
    "        actions = torch.as_tensor(self.action_buffer[idxs], device=self.device).unsqueeze(1).float()\n",
    "        rewards = torch.as_tensor(self.reward_buffer[idxs], device=self.device).unsqueeze(1).float()\n",
    "        dones = torch.as_tensor(self.done_buffer[idxs], device=self.device).unsqueeze(1).float()\n",
    "\n",
    "        for i in range(1, self.length):\n",
    "            next_obses = torch.as_tensor(self.obs_buffer[idxs + i], device=self.device).unsqueeze(1).float()\n",
    "            next_actions = torch.as_tensor(self.action_buffer[idxs + i], device=self.device).unsqueeze(1).float()\n",
    "            next_rewards = torch.as_tensor(self.reward_buffer[idxs + i], device=self.device).unsqueeze(1).float()\n",
    "            next_dones = torch.as_tensor(self.done_buffer[idxs + i], device=self.device).unsqueeze(1).float()\n",
    "            obses = torch.cat((obses, next_obses), 1)\n",
    "            actions = torch.cat((actions, next_actions), 1)\n",
    "            rewards = torch.cat((rewards, next_rewards), 1)\n",
    "            dones = torch.cat((dones, next_dones), 1)\n",
    "\n",
    "        return obses, actions, rewards, dones\n",
    "\n",
    "\n",
    "class CNNDenseModel(nn.Module):\n",
    "    def __init__(self, embed_dim: int, layers: int, h_dim: int,\n",
    "                 activation=nn.ReLU, min=1e-4, max=10.0):\n",
    "        super().__init__()\n",
    "        self._embed_size = embed_dim\n",
    "        self._layers = layers\n",
    "        self._hidden_size = h_dim\n",
    "        self.activation = activation\n",
    "        self.model = self.build_model()\n",
    "        self.soft_plus = nn.Softplus()\n",
    "        self._min = min\n",
    "        self._max = max\n",
    "        self.conv_channels = 4\n",
    "\n",
    "        self.conv_in = nn.Sequential(torch.nn.Conv2d(in_channels=3,\n",
    "                                                     out_channels=self.conv_channels,\n",
    "                                                     kernel_size=3,\n",
    "                                                     stride=3))\n",
    "        self.fc_out = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = [nn.Linear(400, self._hidden_size)]\n",
    "        model += [self.activation()]\n",
    "        for i in range(self._layers - 1):\n",
    "            model += [nn.Linear(self._hidden_size, self._hidden_size)]\n",
    "            model += [self.activation()]\n",
    "        model += [nn.Linear(self._hidden_size, self._embed_size)]\n",
    "        return nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, obs_visual):\n",
    "        x_visual = self.conv_in(obs_visual)\n",
    "        x_visual = x_visual.contiguous()\n",
    "        x_visual = x_visual.view(-1, self.conv_channels * 10 * 10)\n",
    "        x = self.fc_out(x_visual)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNDecoder(torch.nn.Module):\n",
    "    def __init__(self, z_dim=10, h_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv_channels = 4\n",
    "\n",
    "        self.fc = nn.Sequential(torch.nn.Linear(z_dim, h_dim),\n",
    "                                torch.nn.ReLU(),\n",
    "                                torch.nn.Linear(h_dim, self.conv_channels * 10 * 10))\n",
    "        self.deconv = nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(in_channels=self.conv_channels, out_channels=3, kernel_size=5, stride=3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        h = x.view(-1, self.conv_channels, 10, 10)\n",
    "        h = self.deconv(h)  # , output_size=(x.size(0), 3, 28, 28))\n",
    "        return h\n",
    "\n",
    "\n",
    "class DenseModelNormal(nn.Module):\n",
    "    def __init__(self, feature_dim: int, output_shape: tuple, layers: int, h_dim: int, activation=nn.ELU,\n",
    "                 min=1e-4, max=10.0):\n",
    "        super().__init__()\n",
    "        self._output_shape = output_shape\n",
    "        self._layers = layers\n",
    "        self._hidden_size = h_dim\n",
    "        self.activation = activation\n",
    "        # For adjusting pytorch to tensorflow\n",
    "        self._feature_size = feature_dim\n",
    "        # Defining the structure of the NN\n",
    "        self.model = self.build_model()\n",
    "        self.soft_plus = nn.Softplus()\n",
    "\n",
    "        self._min = min\n",
    "        self._max = max\n",
    "\n",
    "    def build_model(self):\n",
    "        model = [nn.Linear(self._feature_size, self._hidden_size)]\n",
    "        model += [self.activation()]\n",
    "        for i in range(self._layers - 1):\n",
    "            model += [nn.Linear(self._hidden_size, self._hidden_size)]\n",
    "            model += [self.activation()]\n",
    "        model += [nn.Linear(self._hidden_size, 2 * int(np.prod(self._output_shape)))]\n",
    "        return nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, features):\n",
    "        dist_inputs = self.model(features)\n",
    "        reshaped_inputs_mean = torch.reshape(dist_inputs[..., :np.prod(self._output_shape)],\n",
    "                                             features.shape[:-1] + self._output_shape)\n",
    "        reshaped_inputs_std = torch.reshape(dist_inputs[..., np.prod(self._output_shape):],\n",
    "                                            features.shape[:-1] + self._output_shape)\n",
    "\n",
    "        reshaped_inputs_std = torch.clamp(self.soft_plus(reshaped_inputs_std), min=self._min, max=self._max)\n",
    "        return td.independent.Independent(td.Normal(reshaped_inputs_mean, reshaped_inputs_std), len(self._output_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de4dfa",
   "metadata": {},
   "source": [
    "## State Space Model\n",
    "Here, we define our State Space Model (SSM). Intuitively, the SSM models an agent that is sequentially taking actions in a world and receiving rewards and visual observations. The observation $x_t$ at time $t$ visual observations are generated from the latent state $z_t$. The model assumes Markovian transitions where the next state is conditioned upon the current state and the action $a_t$ taken by the agent. Upon taking an action, the agent receives reward $r_t$. Given the graphical structure in the figure below, the SSM's joint distribution factorizes as:\n",
    "\n",
    "<img src=\"./pgm.png\" width=400 height=400 />\n",
    "\n",
    "\\begin{align}\n",
    "    &p_\\theta(x_{1:T},r_{1:T},z_{0:T},a_{1:T-1}) = \\prod_{t=1}^{T} p_\\theta(x_{t}|z_{t})p_\\theta(r_{t}|z_{t})p_\\theta(z_{t}|z_{t-1},a_{t-1})p_\\psi(a_{t-1}|z_{t-1})p(z_0)\n",
    "\\end{align}\n",
    "\n",
    "Each of the factorized distributions are modelled using nonlinear functions:\n",
    "\n",
    "* Transitions: $p_\\theta(z_{t}|z_{t-1},a_{t-1}) = p(z_{t}| f_\\theta(z_{t-1},a_{t-1}))$\n",
    "* Observations: $p_\\theta(x_{t}|z_{t}) = p(x_{t}| d_\\theta(z_{t})) $ \n",
    "* Rewards: $p_\\theta(r_{t}|z_{t}) = p(r_{t}| r_\\theta(z_{t})) $\n",
    "\n",
    "where $f_\\theta$, $d^m_\\theta$, $r_\\theta$, and $\\pi_\\psi$ are neural networks parameterized by $\\theta$. Given nonlinearity of these factorized distributions, the posterior distribution $p(z_{1:T}|x_{1:T},a_{1:T-1})$ is intractable. Thus, we approximate it by \n",
    "\n",
    "$$p(z_{1:T}|x_{1:T},a_{1:T-1})\\approx\\hat{q}_{\\phi}(z_{1:T}|x_{1:T},a_{1:T-1})=\\prod_{t=2}^T q_\\phi(z_{t}|g_\\phi(x_{t}, z_{t-1}, a_{t-1}))p(z_1)$$\n",
    "\n",
    "where $q_\\phi$ is modeled as a Gaussian distribution and $g_\\theta(x_{t}, z_{t-1}, a_{t-1})$ is a neural network parameterized by $\\phi$, which is typically called inference network.\n",
    "\n",
    "Given all these distributions and trajectories of the form $\\tau = \\left\\{(x_{t}, a_{t}, r_{t})\\right\\}_{t=1}^{T}$ that samples from the ReplayBuffer, we seek to learn the parameters $\\theta$ and $\\phi$. Because maximum likelihood estimation is intractable in this setting, we optimize the evidence lower bound (ELBO) under the data distribution $p_d$ using a variational distribution $q$ over the latent state variables $z_t$ \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{p_d}[\\mathrm{ELBO}] \\leq \\mathbb{E}_{p_d}[\\log p_\\theta(x_{1:T},r_{1:T}|a_{1:T-1})]\n",
    "\\end{align}\n",
    "\n",
    "where \n",
    "\\begin{align}\n",
    "     \\mathrm{ELBO} = & \\sum_{t=1}^{T}\\Big(\\displaystyle  \\mathop{\\mathbb{E}}_{\\hat{q}_{\\phi}(z_{t})}\\left[\\log p_{\\theta}(x_{t}|z_{t})\\right] +\\displaystyle\\mathop{\\mathbb{E}}_{\\hat{q}_{\\phi}(z_{t})}\\left[\\log p_{\\theta}(r_{t}|z_{t})\\right]  \\nonumber\\\\\n",
    "    &- \\displaystyle\\mathop{\\mathbb{E}}_{\\hat{q}_{\\phi}(z_{t-1})}\\left[\\mathrm{KL}\\left[\\hat{q}_{\\phi}(z_{t}) \\| p_{\\theta}(z_{t}|z_{t-1},a_{t-1})\\right]\\right]\\Big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "777370d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialVAE(nn.Module):\n",
    "    def __init__(self, z_dim=8, action_dim=2, embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.image_embed_net = CNNDenseModel(embed_dim=embed_dim, h_dim=64, layers=2)\n",
    "        self.inference_net = DenseModelNormal(feature_dim=embed_dim + z_dim + action_dim,\n",
    "                                              output_shape=(z_dim,),\n",
    "                                              layers=2,\n",
    "                                              h_dim=32)\n",
    "\n",
    "        self.transition_net = DenseModelNormal(feature_dim=z_dim + action_dim,\n",
    "                                               output_shape=(z_dim,),\n",
    "                                               layers=2,\n",
    "                                               h_dim=32)\n",
    "\n",
    "        self.emission_net = CNNDecoder(z_dim=z_dim, h_dim=32)\n",
    "        self.reward_net = DenseModelNormal(feature_dim=z_dim,\n",
    "                                           output_shape=(1,),\n",
    "                                           layers=2,\n",
    "                                           h_dim=32)\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(params=self.parameters(), lr=0.002)\n",
    "\n",
    "        self.anneal_factor = 2.0\n",
    "\n",
    "    def get_z_post_dist(self, obs, z=None, action=None):\n",
    "\n",
    "        obs_embed = self.image_embed_net(obs)\n",
    "\n",
    "        if len(obs.shape) == 3:\n",
    "            obs_embed = obs_embed[0]\n",
    "\n",
    "        if z is None:\n",
    "            z = torch.zeros(obs_embed.shape[:-1] + (self.z_dim,))\n",
    "\n",
    "        if action is None:\n",
    "            action = torch.zeros(obs_embed.shape[:-1] + (self.action_dim,))\n",
    "        z_post_dist = self.inference_net(torch.cat([obs_embed, z, action], dim=-1))\n",
    "        return z_post_dist\n",
    "\n",
    "    def cross_entropy_planning(self, z):\n",
    "        horizon = 4\n",
    "        sample_size = 100\n",
    "        discount_factor = 0.8\n",
    "\n",
    "        action_samples = torch.randint(low=-1, high=2, size=(sample_size, horizon, self.action_dim))\n",
    "\n",
    "        accumulated_reward = torch.zeros((1, 1))\n",
    "        z_temp = z.clone().unsqueeze(0).repeat(sample_size, 1)\n",
    "        for t in range(horizon):\n",
    "            z_temp_dist = self.transition_net(torch.cat([z_temp, action_samples[:, t]], dim=-1))\n",
    "            z_temp = z_temp_dist.mean\n",
    "\n",
    "            reward = self.reward_net(z_temp).mean\n",
    "            accumulated_reward = accumulated_reward + reward * (discount_factor ** t)\n",
    "\n",
    "        opti_id = torch.argmax(accumulated_reward, dim=0)\n",
    "        opti_action_sample = action_samples[opti_id.item()]\n",
    "        return opti_action_sample[0]\n",
    "\n",
    "    def learn(self, replay_buffer, episode_count):\n",
    "        max_epoch = 200\n",
    "        for i in range(max_epoch):\n",
    "            obs, action, reward, done = replay_buffer.sample()\n",
    "\n",
    "            batch_t = obs.shape[1]\n",
    "\n",
    "            # get latent_state\n",
    "            z_post_rsample = [[]] * batch_t\n",
    "            z_post_mean = [[]] * batch_t\n",
    "            z_post_std = [[]] * batch_t\n",
    "\n",
    "            for t in range(batch_t):\n",
    "                if t == 0:\n",
    "                    z_post_dist = self.get_z_post_dist(obs[:, t])\n",
    "                else:\n",
    "                    z_post_dist = self.get_z_post_dist(obs[:, t], z_post_rsample[t - 1], action[:, t - 1])\n",
    "                z_post_rsample[t] = z_post_dist.rsample()\n",
    "                z_post_mean[t] = z_post_dist.mean\n",
    "                z_post_std[t] = z_post_dist.stddev\n",
    "            z_post_rsample = torch.stack(z_post_rsample, dim=1)\n",
    "            z_post_mean = torch.stack(z_post_mean, dim=1)\n",
    "            z_post_std = torch.stack(z_post_std, dim=1)\n",
    "\n",
    "            z_trans_mean = [[]] * batch_t\n",
    "            z_trans_std = [[]] * batch_t\n",
    "            for t in range(batch_t):\n",
    "                if t == 0:\n",
    "                    z_trans_mean[t] = z_post_mean[:, t]\n",
    "                    z_trans_std[t] = z_post_std[:, t]\n",
    "                else:\n",
    "                    z_trans_dist = self.transition_net(torch.cat([z_post_rsample[:, t - 1], action[:, t]], dim=-1))\n",
    "                    z_trans_mean[t] = z_trans_dist.mean\n",
    "                    z_trans_std[t] = z_trans_dist.stddev\n",
    "            z_trans_mean = torch.stack(z_trans_mean, dim=1)\n",
    "            z_trans_std = torch.stack(z_trans_std, dim=1)\n",
    "\n",
    "            obs_rec = self.emission_net(z_post_rsample)\n",
    "            obs_rec_loss = 100 * torch.square(obs_rec - obs.view(-1, 3, 32, 32)).mean()\n",
    "\n",
    "            reward_dist = self.reward_net(z_post_mean.detach())\n",
    "            reward_rec_loss = -reward_dist.log_prob(reward).mean()\n",
    "\n",
    "            kl_loss = (z_trans_std.log() - z_post_std.log()\n",
    "                       + (z_post_std.pow(2) + (z_trans_mean - z_post_mean).pow(2))\n",
    "                       / (2 * z_trans_std.pow(2) + 1e-5) - 0.5).mean()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = obs_rec_loss + reward_rec_loss + 1.0 * torch.max(kl_loss, torch.ones(1) * self.anneal_factor)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.anneal_factor = np.clip(self.anneal_factor * 0.9, 0.1, 10.0)\n",
    "\n",
    "        print(\n",
    "            f'{episode_count}: obs_loss:{obs_rec_loss.item()}, reward_loss:{reward_rec_loss.item()}, kl_loss:{kl_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb173cf",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "Without loss of generality, we begin at time-step $t-1$ where we have a sample of $z_{t-1}$ and taken a action $a_{t-1}$. At time step $t$, we first recieve an visual observation $x_t$ and reward $r_t$. Then,we pass $z_{t-1}$, $a_{t-1}$ and $x_t$ into the variational distribution $q_\\phi(z_t|z_{t-1}, a_{t-1}, x_t)$ and sample a $z_t$ from $q_\\phi$. Given the sample $z_t$, we optimize the following objective:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{argmax}_{a_{t:t+H-1}} \\ \\ J = \\mathrm{E}_{p(z_{t+1:t+H}\\ \\ \\ \\ |a_{t:t+H-1}\\ \\ \\ \\ ,z_t)}\\left[\\sum_{k=t+1}^{t+H} {\\gamma^tr(z_t)} \\right]\n",
    "\\end{align}\n",
    "\n",
    "where $p(z_{t+1:t+H}|a_{t:t+H-1},z_t)=\\prod_{k=t+1}^{t+H}p_\\theta(z_{k}|f_\\theta(z_{k-1},a_{k-1}))$ We choose action $a_t$ to execute and replan at each time step. After that,environment fowards to the $t+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54029a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(test=False):\n",
    "    env = ParticleEnv()\n",
    "    replay_buffer = ReplayBuffer(obs_shape=(3, 32, 32),\n",
    "                                 action_shape=(2,),\n",
    "                                 reward_shape=(1,),\n",
    "                                 capacity=1000,\n",
    "                                 batch_size=50,\n",
    "                                 length=10)\n",
    "\n",
    "    model = SequentialVAE(z_dim=5, action_dim=2)\n",
    "    if test:\n",
    "        model.load_state_dict(torch.load('./model.pt'))\n",
    "\n",
    "    model.eval()\n",
    "    _, obs, reward = env.reset()\n",
    "\n",
    "    z_post_mean = None\n",
    "    action = None\n",
    "\n",
    "    max_episode = 100\n",
    "    episode_count = 0\n",
    "    episode_data_size = 200\n",
    "\n",
    "    visualize_freq = 5\n",
    "\n",
    "\n",
    "    ims = []\n",
    "    fig, ax = plt.subplots()\n",
    "    while True:\n",
    "        avg_reward = 0.0\n",
    "        for i in range(episode_data_size):\n",
    "            z_post_mean = model.get_z_post_dist(torch.as_tensor(obs).float(), z_post_mean, action).mean\n",
    "\n",
    "            action = model.cross_entropy_planning(z_post_mean)\n",
    "            _, obs, reward, done = env.step(action.detach().numpy())\n",
    "\n",
    "            avg_reward += reward\n",
    "            visualize_image = obs.transpose((1, 2, 0))\n",
    "            if episode_count % visualize_freq == 0:\n",
    "\n",
    "                im = ax.imshow(visualize_image, animated=True)\n",
    "                ims.append([im])\n",
    "\n",
    "            replay_buffer.add(obs, action, reward, np.array([done]))\n",
    "\n",
    "            if done:\n",
    "                _, obs, reward = env.reset()\n",
    "                z_post_mean = None\n",
    "                action = None\n",
    "\n",
    "        avg_reward /= episode_data_size\n",
    "        print(f'avg_reward:{avg_reward}')\n",
    "\n",
    "        if episode_count % visualize_freq == 0:\n",
    "            ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "                                            repeat_delay=1000)\n",
    "            writergif = animation.PillowWriter(fps=30)\n",
    "\n",
    "            if test:\n",
    "                file_name = f'./test_episode_{episode_count}.gif'\n",
    "            else:\n",
    "                file_name = f'./episode_{episode_count}.gif'\n",
    "            ani.save(file_name, writer=writergif)\n",
    "            ims.clear()\n",
    "\n",
    "        if not test:\n",
    "            # train model using collected data\n",
    "            model.train()\n",
    "            model.learn(replay_buffer, episode_count)\n",
    "            model.eval()\n",
    "            torch.save(model.state_dict(), './model.pt')\n",
    "\n",
    "        episode_count += 1\n",
    "        if episode_count > max_episode:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ad8726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_reward:-0.38256699793870363\n",
      "0: obs_loss:1.7802104949951172, reward_loss:-0.9428811073303223, kl_loss:1.165176272392273\n",
      "avg_reward:-0.38967081489607147\n",
      "1: obs_loss:1.4218614101409912, reward_loss:-0.91034996509552, kl_loss:1.3015347719192505\n",
      "avg_reward:-0.3273090250466683\n",
      "2: obs_loss:1.2923067808151245, reward_loss:-1.0265989303588867, kl_loss:1.5137642621994019\n",
      "avg_reward:-0.32336307055400837\n",
      "3: obs_loss:1.2721248865127563, reward_loss:-1.1582387685775757, kl_loss:1.1997597217559814\n",
      "avg_reward:-0.23971112899905983\n",
      "4: obs_loss:1.1961008310317993, reward_loss:-1.254366397857666, kl_loss:1.2024930715560913\n",
      "avg_reward:-0.3180173344662827\n",
      "5: obs_loss:1.1998205184936523, reward_loss:-1.3241875171661377, kl_loss:1.0794649124145508\n",
      "avg_reward:-0.36725186314524655\n",
      "6: obs_loss:1.210439682006836, reward_loss:-1.334482192993164, kl_loss:0.9939008951187134\n",
      "avg_reward:-0.35315265823084063\n",
      "7: obs_loss:1.206045389175415, reward_loss:-1.3584407567977905, kl_loss:0.9151557683944702\n",
      "avg_reward:-0.23613933054095126\n",
      "8: obs_loss:1.144340991973877, reward_loss:-1.7264782190322876, kl_loss:0.829367995262146\n",
      "avg_reward:-0.257780499273821\n",
      "9: obs_loss:1.1763012409210205, reward_loss:-1.6872056722640991, kl_loss:0.7943227291107178\n",
      "avg_reward:-0.3096244340470012\n",
      "10: obs_loss:1.1921552419662476, reward_loss:-1.7765469551086426, kl_loss:0.6952340006828308\n",
      "avg_reward:-0.28851610910845765\n",
      "11: obs_loss:1.1312084197998047, reward_loss:-2.0922324657440186, kl_loss:0.6223173141479492\n",
      "avg_reward:-0.23252116163752767\n",
      "12: obs_loss:1.0537136793136597, reward_loss:-1.9714192152023315, kl_loss:0.5527812242507935\n",
      "avg_reward:-0.21994572743536675\n",
      "13: obs_loss:1.1031310558319092, reward_loss:-2.13642954826355, kl_loss:0.5299771428108215\n",
      "avg_reward:-0.2351537230067595\n",
      "14: obs_loss:1.0247743129730225, reward_loss:-2.135895013809204, kl_loss:0.4533371031284332\n",
      "avg_reward:-0.2639848135954856\n",
      "15: obs_loss:1.128153920173645, reward_loss:-2.269235372543335, kl_loss:0.3937060534954071\n",
      "avg_reward:-0.21989070034774735\n",
      "16: obs_loss:1.080178141593933, reward_loss:-2.189552068710327, kl_loss:0.3596796989440918\n",
      "avg_reward:-0.18207025387373268\n",
      "17: obs_loss:1.0374833345413208, reward_loss:-2.184328317642212, kl_loss:0.3826712369918823\n",
      "avg_reward:-0.20137679759298632\n",
      "18: obs_loss:0.9962535500526428, reward_loss:-2.376580238342285, kl_loss:0.330220103263855\n",
      "avg_reward:-0.22030616615081455\n",
      "19: obs_loss:0.9887673854827881, reward_loss:-2.5193252563476562, kl_loss:0.32988110184669495\n",
      "avg_reward:-0.24188062569497873\n",
      "20: obs_loss:1.0502008199691772, reward_loss:-2.4018454551696777, kl_loss:0.3034374415874481\n",
      "avg_reward:-0.19564963894019793\n",
      "21: obs_loss:0.9777553081512451, reward_loss:-2.485405445098877, kl_loss:0.29919910430908203\n",
      "avg_reward:-0.2533361947340291\n",
      "22: obs_loss:0.9505490064620972, reward_loss:-1.9321085214614868, kl_loss:0.29335176944732666\n",
      "avg_reward:-0.20317865272719637\n",
      "23: obs_loss:0.949547529220581, reward_loss:-2.4169046878814697, kl_loss:0.28749096393585205\n",
      "avg_reward:-0.2146569131095807\n",
      "24: obs_loss:0.9666271209716797, reward_loss:-2.3616855144500732, kl_loss:0.2810831069946289\n",
      "avg_reward:-0.24550638418624068\n",
      "25: obs_loss:0.9239546060562134, reward_loss:-2.4121506214141846, kl_loss:0.2840537130832672\n",
      "avg_reward:-0.2581817955809049\n",
      "26: obs_loss:0.9265215992927551, reward_loss:-2.5390939712524414, kl_loss:0.2812042832374573\n",
      "avg_reward:-0.18593308687843557\n",
      "27: obs_loss:1.0176891088485718, reward_loss:-2.5068447589874268, kl_loss:0.2788265347480774\n",
      "avg_reward:-0.2580443364927263\n",
      "28: obs_loss:0.9160783886909485, reward_loss:-2.4940109252929688, kl_loss:0.2571660280227661\n",
      "avg_reward:-0.19113951305733282\n",
      "29: obs_loss:1.0463836193084717, reward_loss:-2.143315315246582, kl_loss:0.2804035246372223\n",
      "avg_reward:-0.18586489014392207\n",
      "30: obs_loss:0.9979698657989502, reward_loss:-2.3861238956451416, kl_loss:0.28293517231941223\n",
      "avg_reward:-0.23891036261386667\n",
      "31: obs_loss:0.8660228252410889, reward_loss:-2.5656871795654297, kl_loss:0.24145427346229553\n",
      "avg_reward:-0.19232759093360027\n",
      "32: obs_loss:1.0012563467025757, reward_loss:-2.320906639099121, kl_loss:0.2241101861000061\n",
      "avg_reward:-0.1818380586843069\n",
      "33: obs_loss:0.8309189081192017, reward_loss:-2.5311005115509033, kl_loss:0.20263557136058807\n",
      "avg_reward:-0.21697553424420823\n",
      "34: obs_loss:0.8170225620269775, reward_loss:-2.5763940811157227, kl_loss:0.17690004408359528\n",
      "avg_reward:-0.22173230949727832\n",
      "35: obs_loss:0.8725423216819763, reward_loss:-2.4775390625, kl_loss:0.20536589622497559\n",
      "avg_reward:-0.18161668047544943\n",
      "36: obs_loss:0.8180681467056274, reward_loss:-2.7577579021453857, kl_loss:0.19360479712486267\n",
      "avg_reward:-0.18892682069377728\n",
      "37: obs_loss:0.8608975410461426, reward_loss:-2.6208958625793457, kl_loss:0.22923646867275238\n",
      "avg_reward:-0.21660395128884122\n",
      "38: obs_loss:0.9277653694152832, reward_loss:-2.522968292236328, kl_loss:0.18930795788764954\n",
      "avg_reward:-0.1637129789033752\n",
      "39: obs_loss:0.8888475894927979, reward_loss:-2.5602352619171143, kl_loss:0.2416476309299469\n",
      "avg_reward:-0.17436744062314705\n",
      "40: obs_loss:0.8328401446342468, reward_loss:-2.5160655975341797, kl_loss:0.21186642348766327\n",
      "avg_reward:-0.15607947999151434\n",
      "41: obs_loss:0.7902052402496338, reward_loss:-2.826634168624878, kl_loss:0.22067703306674957\n",
      "avg_reward:-0.18692851417750622\n",
      "42: obs_loss:0.8029096722602844, reward_loss:-2.7522518634796143, kl_loss:0.2062593698501587\n",
      "avg_reward:-0.18762946407506284\n",
      "43: obs_loss:0.8218798637390137, reward_loss:-2.687758207321167, kl_loss:0.2108277529478073\n",
      "avg_reward:-0.19780263666848746\n",
      "44: obs_loss:0.82979416847229, reward_loss:-2.385938882827759, kl_loss:0.22358058393001556\n",
      "avg_reward:-0.14962000390589192\n",
      "45: obs_loss:0.7634562253952026, reward_loss:-2.7196853160858154, kl_loss:0.20314954221248627\n",
      "avg_reward:-0.18208725650166221\n",
      "46: obs_loss:0.8252509832382202, reward_loss:-2.714510440826416, kl_loss:0.18639737367630005\n",
      "avg_reward:-0.19335208501932327\n",
      "47: obs_loss:0.7518339157104492, reward_loss:-2.669903516769409, kl_loss:0.18231335282325745\n",
      "avg_reward:-0.18024951349579368\n",
      "48: obs_loss:0.847091794013977, reward_loss:-2.574350357055664, kl_loss:0.19221319258213043\n",
      "avg_reward:-0.15833780189764832\n",
      "49: obs_loss:0.7738226652145386, reward_loss:-2.71804141998291, kl_loss:0.17802952229976654\n",
      "avg_reward:-0.1607237735024235\n",
      "50: obs_loss:0.8211626410484314, reward_loss:-2.432231903076172, kl_loss:0.18891505897045135\n",
      "avg_reward:-0.1826913624905609\n",
      "51: obs_loss:0.8176398277282715, reward_loss:-2.82446551322937, kl_loss:0.20461025834083557\n",
      "avg_reward:-0.16865978891712932\n",
      "52: obs_loss:0.7855435013771057, reward_loss:-2.7369654178619385, kl_loss:0.1852932572364807\n",
      "avg_reward:-0.17838035001323757\n",
      "53: obs_loss:0.8057584762573242, reward_loss:-2.8139584064483643, kl_loss:0.20173844695091248\n",
      "avg_reward:-0.18839674896575428\n",
      "54: obs_loss:0.7985426187515259, reward_loss:-2.8294005393981934, kl_loss:0.19559496641159058\n",
      "avg_reward:-0.1544376636642493\n",
      "55: obs_loss:0.800348699092865, reward_loss:-2.718832492828369, kl_loss:0.2118331491947174\n",
      "avg_reward:-0.1738360865366575\n",
      "56: obs_loss:0.8128490447998047, reward_loss:-2.7741549015045166, kl_loss:0.20415060222148895\n",
      "avg_reward:-0.2197977162182735\n",
      "57: obs_loss:0.7633922100067139, reward_loss:-2.820831298828125, kl_loss:0.19337207078933716\n",
      "avg_reward:-0.2300016986106012\n",
      "58: obs_loss:0.949262261390686, reward_loss:-2.5227208137512207, kl_loss:0.20232023298740387\n",
      "avg_reward:-0.20179197197081258\n",
      "59: obs_loss:0.9642083048820496, reward_loss:-2.506082057952881, kl_loss:0.2025867998600006\n",
      "avg_reward:-0.21416191853408217\n",
      "60: obs_loss:0.871429979801178, reward_loss:-2.5606963634490967, kl_loss:0.21092355251312256\n",
      "avg_reward:-0.18416520843315326\n",
      "61: obs_loss:1.0018328428268433, reward_loss:-2.3845043182373047, kl_loss:0.17961256206035614\n",
      "avg_reward:-0.1747012711312829\n",
      "62: obs_loss:0.9577713012695312, reward_loss:-2.4843802452087402, kl_loss:0.17776916921138763\n",
      "avg_reward:-0.18145397060206736\n",
      "63: obs_loss:0.8406310081481934, reward_loss:-2.8129374980926514, kl_loss:0.1807868480682373\n",
      "avg_reward:-0.19483076690284562\n",
      "64: obs_loss:0.8576576709747314, reward_loss:-2.863950729370117, kl_loss:0.2081892043352127\n",
      "avg_reward:-0.2062286593016188\n",
      "65: obs_loss:0.7902921438217163, reward_loss:-3.0217666625976562, kl_loss:0.21822939813137054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_reward:-0.2256977990268651\n",
      "66: obs_loss:0.8210762143135071, reward_loss:-2.788252353668213, kl_loss:0.21383191645145416\n",
      "avg_reward:-0.2367741315440081\n",
      "67: obs_loss:0.8912028074264526, reward_loss:-2.7393760681152344, kl_loss:0.20012764632701874\n",
      "avg_reward:-0.16906261216995286\n",
      "68: obs_loss:0.8296284079551697, reward_loss:-2.740994453430176, kl_loss:0.20192988216876984\n",
      "avg_reward:-0.19526436500287525\n",
      "69: obs_loss:0.8764277100563049, reward_loss:-2.4745583534240723, kl_loss:0.20335181057453156\n",
      "avg_reward:-0.19017351772996452\n",
      "70: obs_loss:0.8971287608146667, reward_loss:-2.3109493255615234, kl_loss:0.17949500679969788\n",
      "avg_reward:-0.18975638238080514\n",
      "71: obs_loss:0.7982034087181091, reward_loss:-2.648041248321533, kl_loss:0.19071432948112488\n",
      "avg_reward:-0.15279821175929575\n",
      "72: obs_loss:0.7979451417922974, reward_loss:-2.7471089363098145, kl_loss:0.1753120869398117\n",
      "avg_reward:-0.19529785182298376\n",
      "73: obs_loss:0.7682246565818787, reward_loss:-2.911562442779541, kl_loss:0.1936962753534317\n",
      "avg_reward:-0.17610272252963138\n",
      "74: obs_loss:0.8301599621772766, reward_loss:-2.8038289546966553, kl_loss:0.1784401535987854\n",
      "avg_reward:-0.2045674602479017\n",
      "75: obs_loss:0.7869806289672852, reward_loss:-2.890155553817749, kl_loss:0.2224799245595932\n",
      "avg_reward:-0.17975019736862716\n",
      "76: obs_loss:0.7784923315048218, reward_loss:-2.89794659614563, kl_loss:0.1877768188714981\n",
      "avg_reward:-0.18980530513537575\n",
      "77: obs_loss:0.8069248795509338, reward_loss:-2.844484567642212, kl_loss:0.20304475724697113\n",
      "avg_reward:-0.15836243399703706\n",
      "78: obs_loss:0.8000074028968811, reward_loss:-2.850907802581787, kl_loss:0.2155464142560959\n",
      "avg_reward:-0.21948056859834558\n",
      "79: obs_loss:0.7326573729515076, reward_loss:-2.8266141414642334, kl_loss:0.18250413239002228\n",
      "avg_reward:-0.21054372741078628\n",
      "80: obs_loss:0.8377372026443481, reward_loss:-2.7670059204101562, kl_loss:0.22049623727798462\n",
      "avg_reward:-0.21489060097945273\n",
      "81: obs_loss:0.8691295981407166, reward_loss:-2.583926200866699, kl_loss:0.1868947595357895\n",
      "avg_reward:-0.1737413422459883\n",
      "82: obs_loss:0.8159135580062866, reward_loss:-2.8358914852142334, kl_loss:0.17601892352104187\n",
      "avg_reward:-0.21980501569524147\n",
      "83: obs_loss:0.843589186668396, reward_loss:-2.748521566390991, kl_loss:0.17368853092193604\n",
      "avg_reward:-0.22596876436157187\n",
      "84: obs_loss:0.8727798461914062, reward_loss:-2.783888816833496, kl_loss:0.16862362623214722\n",
      "avg_reward:-0.18685784143280418\n",
      "85: obs_loss:0.8468946218490601, reward_loss:-2.5797414779663086, kl_loss:0.19499176740646362\n",
      "avg_reward:-0.20981598006545618\n",
      "86: obs_loss:0.8686522245407104, reward_loss:-2.6795172691345215, kl_loss:0.20291708409786224\n",
      "avg_reward:-0.18917797133839906\n",
      "87: obs_loss:0.8956671953201294, reward_loss:-2.6041011810302734, kl_loss:0.22497332096099854\n",
      "avg_reward:-0.18941314844647347\n",
      "88: obs_loss:0.8127999305725098, reward_loss:-2.682175397872925, kl_loss:0.19764551520347595\n",
      "avg_reward:-0.18260793140569817\n",
      "89: obs_loss:0.8414559364318848, reward_loss:-2.7514286041259766, kl_loss:0.20727434754371643\n",
      "avg_reward:-0.19774985109089677\n",
      "90: obs_loss:0.7882328629493713, reward_loss:-2.922757863998413, kl_loss:0.18280285596847534\n",
      "avg_reward:-0.20160177744813704\n",
      "91: obs_loss:0.8165270090103149, reward_loss:-2.7784423828125, kl_loss:0.20676591992378235\n",
      "avg_reward:-0.18640780518212402\n",
      "92: obs_loss:0.8303614258766174, reward_loss:-2.6970434188842773, kl_loss:0.2039899230003357\n",
      "avg_reward:-0.19036218387662113\n",
      "93: obs_loss:0.8462975025177002, reward_loss:-2.470121383666992, kl_loss:0.17508810758590698\n",
      "avg_reward:-0.16962395855225934\n",
      "94: obs_loss:0.858514130115509, reward_loss:-2.6782524585723877, kl_loss:0.1888464093208313\n",
      "avg_reward:-0.16230479570393574\n",
      "95: obs_loss:0.7969588041305542, reward_loss:-2.7284297943115234, kl_loss:0.1690882295370102\n",
      "avg_reward:-0.17189354235960366\n",
      "96: obs_loss:0.7607192993164062, reward_loss:-2.892408847808838, kl_loss:0.18736155331134796\n",
      "avg_reward:-0.17708528528775166\n",
      "97: obs_loss:0.7073414325714111, reward_loss:-2.9575271606445312, kl_loss:0.174448162317276\n",
      "avg_reward:-0.2232128927845541\n",
      "98: obs_loss:0.8085544109344482, reward_loss:-2.805107831954956, kl_loss:0.19523900747299194\n",
      "avg_reward:-0.23005984433500168\n",
      "99: obs_loss:0.8409008383750916, reward_loss:-2.787243366241455, kl_loss:0.20224319398403168\n",
      "avg_reward:-0.24100605695511468\n",
      "100: obs_loss:0.922562837600708, reward_loss:-2.561633825302124, kl_loss:0.20466716587543488\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13f349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e510714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
